{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning to rank with scikit-learn: the pairwise transform\n",
    "============================================================\n",
    "На основе семинара Никиты Волкова и https://habrahabr.ru/post/263823/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key words:** \n",
    "   задача ранжированя, построение признаков для задачи ранжирования, попарный и списочный подходы, Lemur Project, обратный индекс.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Plan </h3>\n",
    "  * **HW6 discussion** (10 minutes)\n",
    "     \n",
    "  * **Features for ranking ** (20 minutes)\n",
    "  * **RankLib (The Lemur Project)** (40 minutes)\n",
    "  * **Reverse index** (10 minutes)</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features for ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF - классический текстовый признак\n",
    "\n",
    "\n",
    "TF-IDF$(q,d)$ - мера релевантности документа $d$ запросу $q$\n",
    "\n",
    "$n_{dw}$ (term frequency) - число вхождений слова $w$ в текст $d$;\n",
    "\n",
    "$N_{w}$ (document frequency) - число документов, содержащих~$w$;\n",
    "\n",
    "$N$ - число документов в коллекции $D$;\n",
    "\n",
    "\n",
    "$N_{w}/N$ - оценка вероятности встретить слово $w$ в документе;\n",
    "\n",
    "$(N_{w}/N)^{n_{dw}}$ - оценка вероятности встретить его $n_{dw}$ раз;\n",
    "\n",
    "$P(q,d) = \\prod\\limits_{w\\in q} (N_{w}/N)^{n_{dw}}$ -\n",
    "оценка вероятности встретить в~документе $d$ слова запроса $q=\\{w_1,\\dots,w_k\\}$ **чисто случайно**;\n",
    "\n",
    "Оценка релевантности запроса $q$ документу $d$:\n",
    "\n",
    "$$ -\\log P(q,d) = \\sum_{w\\in q}\n",
    "        \\underbrace{n_{dw}\\mathstrut}_{\\text{TF}(w,d)}\n",
    "        \\underbrace{\\log (N/N_{w})}_{\\text{IDF}(w)}\n",
    "        \\;\\; \\to \\;\\; \\max. $$\n",
    "\n",
    "TF$(w,d) = n_{dw}$ - term frequency;\n",
    "\n",
    "IDF$(w)=\\log (N/N_{w})$ - inverted document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank - классический ссылочный признак\n",
    "\n",
    "Документ $d$ тем важнее,\n",
    "- чем больше других документов $c$ ссылаются на $d$,\n",
    "- чем важнее документы $c$, ссылающиеся на $d$,\n",
    "- чем меньше других ссылок имеют эти документы $c$.\n",
    "\n",
    "    \n",
    "Вероятность попасть на страницу $d$, если кликать случайно:\n",
    "    \n",
    "$$PR(d) = \\frac{1-\\delta}N + \\delta\\! \\sum_{c\\in D^{in}_d} \\frac{PR(c)}{|D^{out}_c|},$$\n",
    "    \n",
    "    \n",
    "$D^{in}_d \\subset D$ - множество документов, ссылающихся на $d$,\n",
    "\n",
    "$D^{out}_c \\subset D$ - множество документов, на которые ссылается $c$,\n",
    "\n",
    "$\\delta = 0.85$ - вероятность продолжать клики (damping factor),\n",
    "\n",
    "$N$ - число документов в коллекции $D$.\n",
    "\n",
    "Sergey Brin, Lawrence Page.\n",
    "The Anatomy of a Large-Scale Hypertextual Web Search Engine. 1998.\n",
    "\n",
    "** Упражнение:** Посчитать PageRank для небольшого графа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flaren/anaconda/envs/MyPython2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from scipy import stats, linalg\n",
    "from sklearn import svm, linear_model, cross_validation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('text.latex', unicode=True)\n",
    "plt.rc('text.latex', preamble='\\\\usepackage[utf8]{inputenc}')\n",
    "plt.rc('text.latex', preamble='\\\\usepackage[russian]{babel}')\n",
    "plt.rc('font', family='serif', size='16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RankLib (The Lemur Project)\n",
    "\n",
    "<img src=\"./lemur-big-370.png\">\n",
    "\n",
    "RankLib --- мощная библиотека для обучения ранжированию. Написана на Java. Описание https://sourceforge.net/p/lemur/wiki/RankLib/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-**MART** (градиентный бустинг над регрессионными деревьями) [6]\n",
    "\n",
    "-**RankNet** [1] (попарный подход, гладкий функционал качества, метод стохастического градиента)\n",
    "\n",
    "-**RankBoost** [2] (модификация AdaBoost для классификации пар документов)\n",
    "\n",
    "-**AdaRank** [3] (листовой подход, бустинг, базовые ранкеры - признаки, экспоненциальная функция)\n",
    "\n",
    "-**Coordinate Ascent** (метода координатного подъема) [4]\n",
    "\n",
    "-**LambdaMART** [5] (попарный/листовой, комбинация MART и LambdaRank)\n",
    "\n",
    "-**ListNet** [7] (листовой подход, оптимизация гладкой функции, похожей на целевую, сравниваем распределения истинных меток с порождаемыми)\n",
    "\n",
    "-**Random Forests** [8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем датасет с конкурса «Интернет-математика 2009»\n",
    "https://academy.yandex.ru/events/data_analysis/grant2009/\n",
    "\n",
    "Ниже определены некоторые вспомогательные функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Особенности формата данных:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<line> .=. <target> qid:<qid> <feature>:<value> <feature>:<value> ... <feature>:<value> # <info>\n",
    "<target> .=. <positive integer> метка релевантности\n",
    "<qid> .=. <positive integer> номер запроса\n",
    "<feature> .=. <positive integer>\n",
    "<value> .=. <float>\n",
    "<info> .=. <string>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 qid:1 1:1 2:1 3:0 4:0.2 5:0 # 1A\n",
    "2 qid:1 1:0 2:0 3:1 4:0.1 5:1 # 1B \n",
    "1 qid:1 1:0 2:1 3:0 4:0.4 5:0 # 1C\n",
    "1 qid:1 1:0 2:0 3:1 4:0.3 5:0 # 1D  \n",
    "1 qid:2 1:0 2:0 3:1 4:0.2 5:0 # 2A  \n",
    "2 qid:2 1:1 2:0 3:1 4:0.4 5:0 # 2B \n",
    "1 qid:2 1:0 2:0 3:1 4:0.1 5:0 # 2C \n",
    "1 qid:2 1:0 2:0 3:1 4:0.2 5:0 # 2D  \n",
    "2 qid:3 1:0 2:0 3:1 4:0.1 5:1 # 3A \n",
    "3 qid:3 1:1 2:1 3:0 4:0.3 5:0 # 3B \n",
    "4 qid:3 1:1 2:0 3:0 4:0.4 5:1 # 3C \n",
    "1 qid:3 1:0 2:1 3:1 4:0.5 5:0 # 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все написанное после \"#\" не читается (комментарий)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(file_path, features_count):\n",
    "    ''' Считывает дата-файл по адресу file_path, в котором есть не более features_count признаков.\n",
    "    Возвращает список меток релевантности, id запросов и матрицу признаков'''\n",
    "    \n",
    "    relevs = []\n",
    "    qids = []\n",
    "    features = []\n",
    "    \n",
    "    with open(file_path) as data_file:\n",
    "        for line in data_file:\n",
    "            split_line = line.split(' ')\n",
    "            \n",
    "            # релевантность и id запроса\n",
    "            relevs.append(split_line[0])\n",
    "            qids.append(int(split_line[1].split(':')[1]))\n",
    "            \n",
    "            # признаки\n",
    "            object_features = np.zeros(features_count, dtype=float)\n",
    "            for feat in split_line[2:]:\n",
    "                index, value = map(float, feat.split(':'))\n",
    "                object_features[index] = value\n",
    "            \n",
    "            features.append(object_features)\n",
    "    \n",
    "    return relevs, qids, np.array(features)\n",
    "\n",
    "\n",
    "def write(features, relevs, qids, file_path, index_begin, index_end):\n",
    "    ''' Создает файл по адресу file_path, в который будут записаны релевантности relevs,\n",
    "    номера запросов qids и признаки features с номера index_begin по index_end. '''\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        for index_line in range(index_begin, index_end):\n",
    "            f.write('{} qid:{}'.format(relevs[index_line], qids[index_line]))\n",
    "            for i in range(features.shape[1]):\n",
    "                f.write(' {}:{}'.format(i + 1, features[index_line, i]))\n",
    "            f.write('\\n')\n",
    "            \n",
    "\n",
    "def split_to_train_valid_test(relevs, qids, features, \n",
    "                              train_path, test_path,\n",
    "                              train_size, test_size,\n",
    "                              valid_path=None, valid_size=None):\n",
    "    ''' Разбивает датасет на две или три части, и записывает их в файлы'''\n",
    "\n",
    "    num_docs = len(qids)\n",
    "    i_0 = 0\n",
    "    \n",
    "    qids = np.array(qids)\n",
    "    is_new_query = qids[:-1] != qids[1:]  # True в тех позициях, в которых начинается новый запрос\n",
    "    new_query_positions = np.arange(num_docs - 1)[is_new_query]  # Позиции, в которых начинается новый запрос\n",
    "\n",
    "    # Ищем позицию, на которой заканчивается train\n",
    "    allow_positions = new_query_positions > (train_size * num_docs)\n",
    "    i_1 = new_query_positions[allow_positions][0] if np.sum(allow_positions) > 0 else num_docs\n",
    "\n",
    "    # Ищем позицию, на которой заканчивается test\n",
    "    allow_positions = new_query_positions > ((train_size + test_size) * num_docs)\n",
    "    i_2 = new_query_positions[allow_positions][0] if np.sum(allow_positions) > 0 else num_docs\n",
    "\n",
    "    write(features, relevs, qids, train_path, i_0, i_1)\n",
    "    write(features, relevs, qids, test_path, i_1, i_2)\n",
    "\n",
    "    if valid_path is not None:\n",
    "        # Ищем позицию, на которой заканчивается valid\n",
    "        allow_positions = new_query_positions > ((train_size + test_size + valid_size) * num_docs)\n",
    "        i_3 = new_query_positions[allow_positions][0] if np.sum(allow_positions) > 0 else num_docs\n",
    "\n",
    "        write(features, relevs, qids, valid_path, i_2, i_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relevs, qids, features = read_file('./data/imat2009_learning.txt', 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем его на три части и запишем каждую в файл. Основная проблема --- для RankLib у объекта должны быть указаны все признаки, даже если они равны нулю. Поэтому приходится их искусственно дописывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split_to_train_valid_test(relevs, qids, features,\n",
    "                          './data/train.txt', './data/test.txt', 0.01, 0.01, \n",
    "                          valid_path='./data/valid.txt', valid_size=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим LambdaMART (бустинг на деревьях, модель 6) на 100 деревьях с 5 листами в каждом. Будем использовать метрику $NDCG_{10}$. Сохраняем саму модель в ./model/LambdaMART_100_5.txt, а вывод обучения в ./model/log_LambdaMART_100_5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.4 ms, sys: 35.1 ms, total: 99.5 ms\n",
      "Wall time: 3.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! java -jar RankLib-2.1-patched.jar -train ./data/train.txt -test ./data/test.txt -validate ./data/valid.txt -ranker 6 -tree 100 -leaf 5 -metric2t NDCG@10 -save ./model/LambdaMART_100_5.txt > ./model/log_LambdaMART_100_5.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 деревьев, 10 листев в каждом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 181 ms, sys: 80.1 ms, total: 261 ms\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! java -jar RankLib-2.1-patched.jar -train ./data/train.txt -test ./data/test.txt -validate ./data/valid.txt -ranker 6 -metric2t NDCG@10 -save ./model/LambdaMART_1000_10.txt > ./model/log_LambdaMART_1000_10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоже самое, но без валидационной выборки. То есть он будет строить все 1000 деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 749 ms, sys: 303 ms, total: 1.05 s\n",
      "Wall time: 42.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! java -jar RankLib-2.1-patched.jar -train ./data/train.txt -test ./data/test.txt -ranker 6 -metric2t NDCG@10 -save ./model/LambdaMART_1000_10_novalid.txt > ./model/log_LambdaMART_1000_10_novalid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим RankNet (нейронная сеть) с различными параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "CPU times: user 49.8 ms, sys: 30.1 ms, total: 80 ms\n",
      "Wall time: 2.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! java -jar RankLib-2.1-patched.jar -train ./data/train.txt -test ./data/test.txt -validate ./data/valid.txt -ranker 1 -metric2t NDCG@10 -save ./model/RankNet.txt > ./model/log_RankNet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 s, sys: 738 ms, total: 2.61 s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! java -jar RankLib-2.1-patched.jar -train ./data/train.txt -test ./data/test.txt -validate ./data/valid.txt -ranker 1 -layer 3 -node 30 -metric2t NDCG@10 -save ./model/RankNet_3_30.txt > ./model/log_RankNet_3_30.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Игрушечный\" поисковый движок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть два основный этапа в разработке поискового движка: построение индекса, а затем, используя индекс, ответить на запрос. А затем мы можем добавить результат рейтинга (TF-IDF, PageRank и т.д.), классификацию запрос/документ, и, возможно, немного машинного обучения, чтобы отслеживать последние запросы пользователя и на основе этого выбрать результаты для повышения производительности поисковой системы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение индекса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, первый шаг в построении текстового поискового движка — это сборка перевёрнутого индекса. \n",
    "**Перевёрнутый индекс** — это структура данных, которая сопоставляет маркеры с документами, в который они появляются. В данном контексте мы можем рассматривать маркер просто как слова, таким образом перевёрнутый индекс, в своей основе, это что-то, что берёт слово и возвращает нам список документов, где оно встречается.\n",
    "\n",
    "Необходимо проанализировать и отметить (маркировать, разделив на слова) множество документов. Мы сделаем это следующим образом: для каждого документа, который мы хотим добавить в наш индекс, мы удалим всю пунктуацию и разделить его на пробелы, создадим временную хеш-таблицу, которая соотносит имена документов к списку маркеров.  Вот код, который сделает первоначальную фильтрацию текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_files(self):\n",
    "    file_to_terms = {}\n",
    "    for file in self.filenames:\n",
    "        pattern = re.compile('[\\W_]+')\n",
    "        file_to_terms[file] = open(file, 'r').read().lower();\n",
    "        file_to_terms[file] = pattern.sub(' ',file_to_terms[file])\n",
    "        re.sub(r'[\\W_]+','', file_to_terms[file])\n",
    "        file_to_terms[file] = file_to_terms[file].split()\n",
    "    return file_to_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перевёрнутый индекс будет картой слова до имени документа, но мы так же хотим поддерживать запросы с фразами: запросы не только для слов, но и для слов в определённой последовательности. Для этого мы должны знать где в документе появляется каждое слово, таким образом мы сможем проверить порядок слов. Я индекс каждого слова в маркированном списке на документ в качестве позиции слова в этом документе, поэтому наш конечный перевернутый индекс будет выглядеть следующим образом:\n",
    "\n",
    "**{word: {documentID: [pos1, pos2, ...]}, ...}, ...}**\n",
    "\n",
    "вместо такого:\n",
    "\n",
    "**{word: [documentID, ...], ...}**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, наша первая задача состоит в том, чтобы создать сопоставление слов для своих позиций для каждого документа, а затем объединить их, чтобы создать наш полный перевёрнутый индекс. Это выглядит как:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input = [word1, word2, ...]\n",
    "#output = {word1: [pos1, pos2], word2: [pos2, pos434], ...}\n",
    "def index_one_file(termlist):\n",
    "    fileIndex = {}\n",
    "    for index, word in enumerate(termlist):\n",
    "        if word in fileIndex.keys():\n",
    "            fileIndex[word].append(index)\n",
    "        else:\n",
    "            fileIndex[word] = [index]\n",
    "    return fileIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код принимает список терминов в документе, разделённые пробелом (в котором слова находятся в их первоначальном порядке), и добавляет каждое в хеш-таблицу, где значением является список позиций этого слова в документе. Мы строим этот список многократно, как мы идём по списку, до тех пор, пока не пройдём все слова, оставив нас с таблицей, снабжённую ключами по строкам и размеченными до списка позиций этих строк.\n",
    "\n",
    "Теперь нам нужно объединить эти хеш-таблицы. \n",
    "\n",
    "Промежуточный формат индекса:\n",
    "\n",
    "**{documentID: {word: [pos1, pos2, ...]}, ...}**\n",
    "\n",
    "который преобразуется к окончательному формату:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input = {filename: [word1, word2, ...], ...}\n",
    "#res = {filename: {word: [pos1, pos2, ...]}, ...}\n",
    "def make_indices(termlists):\n",
    "    total = {}\n",
    "    for filename in termlists.keys():\n",
    "        total[filename] = index_one_file(termlists[filename])\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "данный код принимает результаты функции file_to_terms, и создает новую хеш-таблицу помеченных ключом по имени файла и со значениями, которые являются результатом предыдущей функции, создавая вложенную хеш-таблицу.\n",
    "\n",
    "Строим перевернутый индекс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input = {filename: {word: [pos1, pos2, ...], ... }}\n",
    "#res = {word: {filename: [pos1, pos2]}, ...}, ...}\n",
    "def fullIndex(regdex):\n",
    "    total_index = {}\n",
    "    for filename in regdex.keys():\n",
    "        for word in regdex[filename].keys():\n",
    "            if word in total_index.keys():\n",
    "                if filename in total_index[word].keys():\n",
    "                    total_index[word][filename].extend(regdex[filename][word][:])\n",
    "                else:\n",
    "                    total_index[word][filename] = regdex[filename][word]\n",
    "            else:\n",
    "                total_index[word] = {filename: regdex[filename][word]}\n",
    "    return total_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем ввести слово, и должны получить перечень документов, в которых оно встречается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выполнение запросов к индексу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, есть два типа запросов, которые мы хотим обрабатывать: стандартные запросы, где по крайней мере одно из слов в запросе появляется в документе и запросы с фразой, где все слова запроса встречаются в документе в том же порядке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы собираемся реализовать стандартные запросы в первую очередь. Простой способ реализовать их — разбить запрос на слова (маркеры, как описано выше), получить список за каждое слово, документы в которых они встречаются, а затем объединить все эти списки. Вот как мы выполним запрос для одного слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_word_query(self, word):\n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    word = pattern.sub(' ',word)\n",
    "    if word in self.invertedIndex.keys():\n",
    "        return [filename for filename in self.invertedIndex[word].keys()]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь стандартный запрос является очень простым расширением. Мы просто агрегируем и объединяем списки как показано здесь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def free_text_query(self, string):\n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    string = pattern.sub(' ',string)\n",
    "    result = []\n",
    "    for word in string.split():\n",
    "        result += self.one_word_query(word)\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последним типом запросов является запрос с фразой, который немного сложнее, так как мы должны гарантировать правильный порядок слов в документах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phrase_query(self, string):\n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    string = pattern.sub(' ',string)\n",
    "    listOfLists, result = [],[]\n",
    "    for word in string.split():\n",
    "        listOfLists.append(self.one_word_query(word))\n",
    "    setted = set(listOfLists[0]).intersection(*listOfLists)\n",
    "    for filename in setted:\n",
    "        temp = []\n",
    "        for word in string.split():\n",
    "            temp.append(self.invertedIndex[word][filename][:])\n",
    "        for i in range(len(temp)):\n",
    "            for ind in range(len(temp[i])):\n",
    "                temp[i][ind] -= i\n",
    "        if set(temp[0]).intersection(*temp):\n",
    "            result.append(filename)\n",
    "    return self.rankResults(result, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И так, мы вновь сначала обрабатываем текст входного запроса. Затем мы запускаем одно слово из запроса для каждого слова на входе, и добавляем каждый из этих списков результатов в наш общий список. Затем мы создаём множество с именем 'setted', который принимает пересечение первого списка со всеми другими списками (по существу, принимая пересечение всех списков), и оставляет нас с промежуточным результатом: множеством всех документов, содержащих все слова запроса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ранжирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Заключительным шагом в построении поискового движка является создание системы для ранжирования документов по их релевантности к запросу. Это наиболее сложная часть, поскольку она не имеет прямого технического решения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "В версии, представленной в папке Text-Search-Engine реализовано TF-IDF ранжирование. Рекомендуется ознакомиться с финальной версией движка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Conclusion </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "** Check questions **\n",
    "\n",
    "1) Чем отличаются поточечный и попарный подходы к ранжированию? Какой лучше работает и почему?\n",
    "\n",
    "2) Чем отличается RankSVM от обычного SVM?\n",
    "\n",
    "3) Что такое коэффициент корреляции Кенделла? Как его использовать для оценки качества модели?\n",
    "\n",
    "4) Какие алгоритмы ранжирования реализованные в RankLib проекта Lemur мы сегодня рассматривали? \n",
    "\n",
    "5) Какие этапы построения простейшего поискового движка можно выделить? В чем их сложность?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton and G. Hullender. Learning to rank using gradient descent. In Proc. of ICML, pages 89-96, 2005.\n",
    "\n",
    "[2] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. The Journal of Machine Learning Research, 4: 933-969, 2003.\n",
    "\n",
    "[3] J. Xu and H. Li. AdaRank: a boosting algorithm for information retrieval. In Proc. of SIGIR, pages 391-398, 2007.\n",
    "\n",
    "[4] D. Metzler and W.B. Croft. Linear feature-based models for information retrieval. Information Retrieval, 10(3): 257-274, 2007.\n",
    "\n",
    "[5] Q. Wu, C.J.C. Burges, K. Svore and J. Gao. Adapting Boosting for Information Retrieval Measures. Journal of Information Retrieval, 2007.\n",
    "\n",
    "[6] J.H. Friedman. Greedy function approximation: A gradient boosting machine. Technical Report, IMS Reitz Lecture, Stanford, 1999; see also Annals of Statistics, 2001.\n",
    "\n",
    "[7] Z. Cao, T. Qin, T.Y. Liu, M. Tsai and H. Li. Learning to Rank: From Pairwise Approach to Listwise Approach. ICML 2007. \n",
    "\n",
    "[8] L. Breiman. Random Forests. Machine Learning 45 (1): 5–32, 2001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** You can find HW7 here** \n",
    "  * оцените <a href=\"https://goo.gl/forms/td2EbpfpAQBQNfme2\"> семинар </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
